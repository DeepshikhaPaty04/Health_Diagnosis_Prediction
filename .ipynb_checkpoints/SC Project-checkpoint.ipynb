{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ff07b6-bff8-48cd-b3e7-70aaac805069",
   "metadata": {},
   "source": [
    "<h1><center>Soft Computing Project</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d21e041-df88-4f00-a84f-c0647dc2f448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMI                   2.141479e-18\n",
      "Smoking              -4.068810e-17\n",
      "AlcoholConsumption    2.569775e-17\n",
      "PhysicalActivity     -5.728456e-17\n",
      "DietQuality           1.638231e-16\n",
      "SleepQuality         -5.889067e-17\n",
      "dtype: float64\n",
      "BMI                   1.0\n",
      "Smoking               1.0\n",
      "AlcoholConsumption    1.0\n",
      "PhysicalActivity      1.0\n",
      "DietQuality           1.0\n",
      "SleepQuality          1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from CSV\n",
    "df = pd.read_csv('SC_project.csv')\n",
    "\n",
    "# Extract features and target\n",
    "X = df.drop(columns=['PatientID', 'Diagnosis'])  # Exclude PatientID and Diagnosis\n",
    "y = df['Diagnosis']\n",
    "\n",
    "# Normalize the features\n",
    "X = (X - X.mean()) / X.std()  # Normalize using mean and std\n",
    "print(X.mean())\n",
    "print(X.std())\n",
    "\n",
    "# Convert features and target to numpy arrays\n",
    "X = X.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b3249b6-d835-4598-aec8-8bd96a71dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Weights = [ 0.04278792  0.0254288   0.00535678 -0.00797237 -0.08794525  0.0452724 ], Bias = [0.66041981]\n",
      "Epoch 2: Weights = [ 0.04278786  0.02542882  0.0053567  -0.00797235 -0.08794527  0.04527244], Bias = [0.66041985]\n",
      "Epoch 3: Weights = [ 0.04278786  0.02542882  0.0053567  -0.00797235 -0.08794527  0.04527244], Bias = [0.66041985]\n",
      "Epoch 4: Weights = [ 0.04278786  0.02542882  0.0053567  -0.00797235 -0.08794527  0.04527244], Bias = [0.66041985]\n",
      "Epoch 5: Weights = [ 0.04278786  0.02542882  0.0053567  -0.00797235 -0.08794527  0.04527244], Bias = [0.66041985]\n",
      "\n",
      "Final Weights and Bias:\n",
      "Weights: [ 0.04278786  0.02542882  0.0053567  -0.00797235 -0.08794527  0.04527244]\n",
      "Bias: [0.66041985]\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "# Use single layer feed forward network and find the weight values. (ADALINE)\n",
    "\n",
    "# Initialize parameters\n",
    "n_samples, n_features = X.shape\n",
    "weights = np.random.rand(n_features)  # Initialize weights --> 1D arrays, randomly, length= n_features\n",
    "bias = np.random.rand(1)  # Initialize bias randomly\n",
    "learning_rate = 0.01 \n",
    "epochs = 5\n",
    "\n",
    "# ADALINE Training Loop\n",
    "for epoch in range(epochs):    #loop will run 5 times, once for each training cycle over the dataset.\n",
    "    for i in range(n_samples):\n",
    "        # Forward pass: Weighted sum\n",
    "        y_pred = np.dot(X[i], weights) + bias\n",
    "        \n",
    "        # Error calculation\n",
    "        error = y[i] - y_pred\n",
    "        \n",
    "        # Weight and bias update (LMS Rule)\n",
    "        weights += learning_rate * error * X[i]\n",
    "        bias += learning_rate * error\n",
    "    \n",
    "    # Display weights after each epoch\n",
    "    print(f\"Epoch {epoch + 1}: Weights = {weights}, Bias = {bias}\")\n",
    "\n",
    "# Final weight values\n",
    "print(\"\\nFinal Weights and Bias:\")\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Bias:\", bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8bb9eae-c4a3-4c5a-84c4-12bec3a669da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: \n",
      "Weights (Input to Hidden) = \n",
      "[[0.05918253 0.28929498 0.55498457 0.04011023 0.65977687 0.31120067\n",
      "  0.03991407 0.99803573]\n",
      " [0.97813013 0.04762362 0.80377341 0.71607987 0.34945425 0.511186\n",
      "  0.31382115 0.84693545]\n",
      " [0.01765508 0.50684391 0.57120564 0.03544193 0.54644393 0.44119787\n",
      "  0.14505122 0.17956023]\n",
      " [0.04934639 0.17500197 0.55963163 0.72052881 0.81759947 0.1478959\n",
      "  0.55312517 0.81777073]\n",
      " [0.19856512 0.09597993 0.64486924 0.4707534  0.31869648 0.26989697\n",
      "  0.24568199 0.11618821]\n",
      " [0.85682258 0.88051593 0.31291981 0.61962977 0.06925695 0.10710395\n",
      "  0.70851489 0.35229114]], \n",
      "Weights (Hidden to Output) = \n",
      "[[0.04405656]\n",
      " [0.27918702]\n",
      " [0.0752036 ]\n",
      " [0.13245934]\n",
      " [0.09050519]\n",
      " [0.90677377]\n",
      " [0.7593741 ]\n",
      " [0.4255722 ]]\n",
      "\n",
      "\n",
      "Epoch 2: \n",
      "Weights (Input to Hidden) = \n",
      "[[0.05889417 0.28527203 0.55620446 0.03709004 0.66206941 0.311092\n",
      "  0.03132431 1.01319514]\n",
      " [0.97754315 0.02868003 0.80071214 0.71017543 0.34208687 0.47472328\n",
      "  0.26421141 0.82650889]\n",
      " [0.01665232 0.50400545 0.57053231 0.02874859 0.545142   0.43064995\n",
      "  0.12044138 0.16264764]\n",
      " [0.04678362 0.16017574 0.55600665 0.71691262 0.81479866 0.09386684\n",
      "  0.52622959 0.80127713]\n",
      " [0.1982573  0.08156852 0.64595276 0.46801264 0.31452171 0.24195557\n",
      "  0.21090037 0.09440532]\n",
      " [0.85664508 0.87609192 0.30876844 0.61523718 0.06248002 0.0641096\n",
      "  0.69660113 0.33033781]], \n",
      "Weights (Hidden to Output) = \n",
      "[[-0.02319764]\n",
      " [ 0.23398   ]\n",
      " [-0.05605251]\n",
      " [ 0.01057618]\n",
      " [ 0.01514442]\n",
      " [ 0.85828095]\n",
      " [ 0.68405832]\n",
      " [ 0.34214281]]\n",
      "\n",
      "\n",
      "Epoch 3: \n",
      "Weights (Input to Hidden) = \n",
      "[[0.0593624  0.28190168 0.55566959 0.03700676 0.66305962 0.31237371\n",
      "  0.02150202 1.02675569]\n",
      " [0.97806233 0.014469   0.80186183 0.70935387 0.33949567 0.44936631\n",
      "  0.22327968 0.81279981]\n",
      " [0.01723258 0.50219864 0.57068221 0.02783018 0.54482974 0.4257533\n",
      "  0.09966051 0.14790959]\n",
      " [0.04758543 0.15064212 0.55832472 0.71652482 0.81416553 0.0503714\n",
      "  0.50948484 0.79131869]\n",
      " [0.19929113 0.07117769 0.64895945 0.46923314 0.31352182 0.22288967\n",
      "  0.18796628 0.07864503]\n",
      " [0.85758332 0.8741312  0.31223531 0.61493845 0.06012156 0.02670814\n",
      "  0.69008875 0.31319417]], \n",
      "Weights (Hidden to Output) = \n",
      "[[-0.06497453]\n",
      " [ 0.20772249]\n",
      " [-0.1352501 ]\n",
      " [-0.06271197]\n",
      " [-0.02272635]\n",
      " [ 0.83085504]\n",
      " [ 0.63806253]\n",
      " [ 0.29770261]]\n",
      "\n",
      "\n",
      "Epoch 4: \n",
      "Weights (Input to Hidden) = \n",
      "[[ 0.06023249  0.27930678  0.55494409  0.03819174  0.66341643  0.30830415\n",
      "   0.01479246  1.03805167]\n",
      " [ 0.97822308  0.00365765  0.80367706  0.70986891  0.33865865  0.43845911\n",
      "   0.193307    0.80365452]\n",
      " [ 0.0185992   0.50051472  0.57097388  0.0296203   0.54486883  0.42189419\n",
      "   0.0816875   0.1355962 ]\n",
      " [ 0.04981265  0.14408795  0.56199422  0.71698511  0.81395553  0.01161563\n",
      "   0.50270532  0.78525327]\n",
      " [ 0.20056594  0.06309415  0.65212275  0.47170376  0.31354315  0.20627881\n",
      "   0.1728472   0.06551388]\n",
      " [ 0.85879201  0.87338458  0.31912711  0.61646981  0.05959857 -0.01089958\n",
      "   0.68418622  0.29970281]], \n",
      "Weights (Hidden to Output) = \n",
      "[[-0.08918069]\n",
      " [ 0.1914864 ]\n",
      " [-0.18312239]\n",
      " [-0.10610599]\n",
      " [-0.04100811]\n",
      " [ 0.81644887]\n",
      " [ 0.61036979]\n",
      " [ 0.27825227]]\n",
      "\n",
      "\n",
      "Epoch 5: \n",
      "Weights (Input to Hidden) = \n",
      "[[ 0.06121205  0.27760564  0.55404259  0.03969737  0.66350425  0.30171998\n",
      "   0.01448285  1.04809185]\n",
      " [ 0.97795022 -0.00427323  0.80475153  0.71037707  0.33835853  0.43166876\n",
      "   0.16992929  0.79646396]\n",
      " [ 0.02025029  0.49859021  0.57152048  0.0325731   0.54508938  0.41848339\n",
      "   0.06350367  0.12423247]\n",
      " [ 0.05239089  0.14026359  0.56571935  0.71759129  0.81377374 -0.02272769\n",
      "   0.49943282  0.78138377]\n",
      " [ 0.20163918  0.05613298  0.65509341  0.47453586  0.31392385  0.19221956\n",
      "   0.15932947  0.05473959]\n",
      " [ 0.85998001  0.87311367  0.32672859  0.61866277  0.059706   -0.04609395\n",
      "   0.68081913  0.28854146]], \n",
      "Weights (Hidden to Output) = \n",
      "[[-0.10380973]\n",
      " [ 0.17956337]\n",
      " [-0.21617531]\n",
      " [-0.1348068 ]\n",
      " [-0.05206535]\n",
      " [ 0.80895217]\n",
      " [ 0.59236302]\n",
      " [ 0.27128627]]\n",
      "\n",
      "\n",
      "Final Weights and Biases:\n",
      "Input to Hidden Weights: [[ 0.06121205  0.27760564  0.55404259  0.03969737  0.66350425  0.30171998\n",
      "   0.01448285  1.04809185]\n",
      " [ 0.97795022 -0.00427323  0.80475153  0.71037707  0.33835853  0.43166876\n",
      "   0.16992929  0.79646396]\n",
      " [ 0.02025029  0.49859021  0.57152048  0.0325731   0.54508938  0.41848339\n",
      "   0.06350367  0.12423247]\n",
      " [ 0.05239089  0.14026359  0.56571935  0.71759129  0.81377374 -0.02272769\n",
      "   0.49943282  0.78138377]\n",
      " [ 0.20163918  0.05613298  0.65509341  0.47453586  0.31392385  0.19221956\n",
      "   0.15932947  0.05473959]\n",
      " [ 0.85998001  0.87311367  0.32672859  0.61866277  0.059706   -0.04609395\n",
      "   0.68081913  0.28854146]]\n",
      "Hidden Biases: [0.02079458 0.21832858 0.88727762 0.79913148 0.64882769 0.11661517\n",
      " 0.28119018 0.38131041]\n",
      "Hidden to Output Weights: [[-0.10380973]\n",
      " [ 0.17956337]\n",
      " [-0.21617531]\n",
      " [-0.1348068 ]\n",
      " [-0.05206535]\n",
      " [ 0.80895217]\n",
      " [ 0.59236302]\n",
      " [ 0.27128627]]\n",
      "Output Bias: [1.50775586]\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "# Use multilayer neural network and find the weight values after 5 epoch.\n",
    "\n",
    "# Initialize parameters\n",
    "n_samples, n_features = X.shape\n",
    "hidden_neurons = 8  # Number of neurons in the hidden layer\n",
    "output_neurons = 1  # Binary classification\n",
    "\n",
    "# Weight initialization\n",
    "weights_input_hidden = np.random.rand(n_features, hidden_neurons)  # weights connecting input to the hidden layer.\n",
    "bias_hidden = np.random.rand(hidden_neurons)  # biases for the hidden layer.\n",
    "weights_hidden_output = np.random.rand(hidden_neurons, output_neurons) # weights connecting hidden to output layer.\n",
    "bias_output = np.random.rand(output_neurons)  #biases for the output layer\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "\n",
    "# Activation function (ReLU and Sigmoid)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(n_samples):\n",
    "        # Forward pass\n",
    "        hidden_input = np.dot(X[i], weights_input_hidden) + bias_hidden\n",
    "        hidden_output = relu(hidden_input)\n",
    "        \n",
    "        final_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "        final_output = sigmoid(final_input)\n",
    "        \n",
    "        # Error calculation\n",
    "        error = y[i] - final_output\n",
    "        \n",
    "        # Backpropagation\n",
    "        # Output layer gradients\n",
    "        d_output = error * sigmoid_derivative(final_output)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        d_hidden = d_output.dot(weights_hidden_output.T) * relu_derivative(hidden_input)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        weights_hidden_output += learning_rate * np.outer(hidden_output, d_output)\n",
    "        bias_output += learning_rate * d_output\n",
    "        \n",
    "        weights_input_hidden += learning_rate * np.outer(X[i], d_hidden)\n",
    "        bias_hidden += learning_rate * d_hidden\n",
    "    \n",
    "    # Display weights after each epoch\n",
    "    print(f\"\\nEpoch {epoch + 1}: \\nWeights (Input to Hidden) = \\n{weights_input_hidden}, \\nWeights (Hidden to Output) = \\n{weights_hidden_output}\\n\")\n",
    "\n",
    "# Final weight values\n",
    "print(\"\\nFinal Weights and Biases:\")\n",
    "print(\"Input to Hidden Weights:\", weights_input_hidden)\n",
    "print(\"Hidden Biases:\", bias_hidden)\n",
    "print(\"Hidden to Output Weights:\", weights_hidden_output)\n",
    "print(\"Output Bias:\", bias_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
